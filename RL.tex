\documentclass[9pt]{article}

\usepackage{notes}

\setcitestyle{authoryear}

\begin{document}
\title{Notes on Reinforcement Learning}
\author{Ermo Wei}
\date{}

\maketitle

\tableofcontents
\hypersetup{colorlinks=blue}

\clearpage 

\newcommand{\lackcite}

\begin{topic}{activation}{Activation Function} 
Activation Function are important for neural network to have non-linearity. The commonly used activation function are summarize below. Most of them are typically combined with an affine transformation $\mat{a} = \mat{b} + \mat{W}\mat{x}$ and applied element-wise.
\[
	\mat{h} = f(\mat{a}) \rightarrow h_i = f(a_i) = f(b_i + \mat{W_{i,:}} *\mat{x})
\]
\begin{itemize}
\item Sigmoid\\
\[
f(a) = \frac{1}{1+e^{-a}}
\]
\item Hyperbolic tangent
\[
f(a) = \tanh(a)
\]
\item Softmax
\item Rectifier or rectified linear unit (ReLU)
\[
f(a) = \max(0,a)
\]
\item Maxout
\item Softplus\\
A smooth version of the rectifier, basically the same shape. But this function has differentiability and non-zero derivative everywhere.
\[
f(a) = \log(1+e^a)
\]
\end{itemize}
\end{topic}

\begin{topic}{actorcritic}{Actor-Critic Method}
\Working
\end{topic}

\begin{topic}{approxinfer}{Approximate Inference}
A central task in the application of Bayesian probabilistic models is the evaluation of the posterior distribution $p(\mat{Z}|\mat{X})$ of the latent variables $\mat{Z}$ given the observed data variables $\mat{X}$, and the evaluation of expectations computed with respect to this distribution. However, for many models of practical interest, it will be infeasible to evaluate the posterior distribution or indeed to compute expectations with respect to this distribution. There are several reasons for this:
\begin{itemize}
\item The dimensionality of the latent space is too high to work with directly
\item The posterior distribution has a highly complex form for which expectations are not analytically tractable
\end{itemize}
For example, in the case of continuous variables, the required integrations may not have closed-form analytical solutions, while the dimensionality of the space and the complexity of the integrand may prohibit numerical integration. For discrete variables, the marginalizations involve summing over all possible configurations of the hidden variables, and though this is always possible in principle, we often find in practice that there may be exponentially many hidden states so that exact calculation is prohibitively expensive.

In such situations, we need to resort to approximation schemes, and these fall broadly into two classes, according to whether they rely on deterministic or stochastic approximations. For the deterministic approximation schemes, \linktopic{variational}{variational inference} methods which use global criteria are widely used, some of which scale well to large applications. These methods are basing on analytical approximations to the posterior distribution, for example by assuming that it factorizes in a particular way or that it has a specific parametric form such as a Gaussian. Due to this reasons, they can never generate exact results.

On the other hand, stochastic techniques, which based on numerical \linktopic{basicsampling}{sampling}, such as \linktopic{mcmc}{Markov chain Monte Carlo}, generally have the property that given infinite computational resource, they can generate exact results, and the approximation arises from the use of a finite amount of processor time. In practice, sampling methods can be computationally demanding, often limiting their use to small-scale problems. Also, it can be difficult to know whether a sampling scheme is generating independent samples from the required distribution.
\end{topic}


\begin{topic}{bp}{Back Propagation}
Let's start with linear neurons (also called linear filters). Here we first make two assumptions:
\begin{itemize}
\item The neuron has a real- valued output which is a weighted sum of its inputs
\item The aim of learning is to minimize the square error summed over all training cases.
\end{itemize}  
The model of the linear neuron is like this :
\[
y = \sum w_i x_i = \mat{w}^T\mat{x}
\]
Since we are dealing with a simple linear neuron, we first derive the special case of back propagation algorithm, called \textit{delta rule}, which is a gradient descent learning rule for updating the weights of single layer neural network.
\begin{enumerate}
\item We first define the error of our training cases:
\[
	E = \frac{1}{2} \sum_{n \in \text{training set}} (t^n-y^n)^2
\]
\item Now we take the derivative of the error with respect to the weights
\[
\begin{split}
	\frac{\partial E}{\partial w_i} = &\ \frac{1}{2}\sum_n \frac{dE^n}{dy^n}\frac{\partial y^n}{\partial w_i} \\
	= & - \sum_n x_i^n (t^n-y^n)
\end{split}
\]
\item The \textbf{batch} delta rule changes the weights in proportion to their error derivatives summed over all training cases
\[
	\Delta w_i = - \epsilon \frac{\partial E}{\partial w_i} = \sum_n \epsilon x_i^n (t^n-y^n)
\]
\end{enumerate}

One thing to be notice here, we can get as close as we desire to the best answer by making the learning rate small enough

Ok, since we have the simplest neuron, let's making things little interesting by adding the logistic function. And now, our model become this:
\[
\begin{split}
z = &\ b + \sum_i w_i x_i\\
y = &\ \frac{1}{1+e^{-z}}
\end{split}
\]
their corresponding derivatives are 
\[
\begin{split}
\frac{\partial z}{\partial w_i} = \ x_i  \,\text{,}&\, \frac{\partial z}{\partial x_i} = \ w_i \\
\frac{dy}{dz} = &\ y(1-y)
\end{split}
\]
Thus, take the derivative of $y$ with respect to $w_i$ is
\[
\frac{\partial y}{\partial w_i} = \frac{dy}{dz} \frac{\partial z}{\partial w_i} = x_i y (1-y)
\]
and 
\[
\frac{\partial E}{\partial w_i} = \sum \frac{\partial E}{\partial y^n} \frac{\partial y^n}{\partial w_i} = -\sum_n x_i^n y^n(1-y^n)(t^n-y^n)
\]
In this equation, the first and last terms come from the delta rule, and the term in the middle is the slope of logistic

\Working
\end{topic}

\begin{topic}{basicsampling}{Basic Sampling}
In the application of Bayesian probabilistic models, we often need to evaluate the posterior distribution $p(\mat{Z}|\mat{X})$ of the latent variables $\mat{Z}$ given the observed data variables $\mat{X}$ (see \linktopic{approxinfer}{Approximate Inference}).

Although for some applications the posterior distribution over unobserved variables will be of direct interest in itself, for most situations the posterior distribution is required primarily for the purpose of evaluating expectations, for example in order to make predictions. The fundamental problem that we therefore wish to address involves finding the expectation of some function $f(\mat{z})$ with respect to a probability distribution $p(\mat{z})$. Here, the components of $\mat{z}$ might comprise discrete or continuous variables or some combination of the two. Thus in the case of continuous variables, we wish to evaluate the expectation
\begin{equation}
\label{eq:expectation}
\E[f] = \int f(\mat{z})p(\mat{z})\D \mat{z}
\end{equation}
where the integral is replaced by summation in the case of discrete variables.  We shall suppose that such expectations are too complex to be evaluated exactly using analytical techniques.

The general idea behind sampling methods is to obtain a set of samples $\mat{z}^{(l)}$ (where $l = 1,\ldots, L$) drawn independently from the distribution $p(\mat{z})$. This allows the expectation~(\ref{eq:expectation}) to be approximated by a finite sum
\[
\hat{f} = \frac{1}{L}\sum_{l=1}^{L} f(\mat{z}^{(l)})
\]
As long as the samples $\mat{z}^{(l)}$ are drawn from the distribution $p(\mat{z})$, then $\E[\hat{f}] = \E[f]$ and so the estimator $\hat{f}$ has the correct mean. The variance of the estimator is given by
\[
	\text{Var}[\hat{f}] = \frac{1}{L}\E[(f-\E[f])^2]
\]
is the variance of the function $f(\mat{z})$ under the distribution $p(\mat{z})$. It is worth emphasizing that the accuracy of the estimator therefore does not depend on the dimensionality of $\mat{z}$, and that, in principle, high accuracy may be achievable with a relatively small number of samples $\mat{z}^{(l)}$. In practice, ten or twenty independent samples may suffice to estimate an expectation to sufficient accuracy.

The problem, however, is that the samples $\{\mat{z}^{(l)}\}$ might not be independent, and so the effective sample size might be much smaller than the apparent sample size. Also, we note that if $f(\mat{z})$ is small in regions where $p(\mat{z})$ is large, and vice versa, then the expectation may be dominated by regions of small probability, implying that relatively large sample sizes will be required to achieve sufficient accuracy.

We first consider how to sample from some common distributions. We assume that we have a pseudo-random generator which can generate numbers distributed uniformly over $(0,1)$.

\Working


\end{topic}

\begin{topic}{bellmanequation}{Bellman Equation}
Due to the Markov property of MDP, we can define the value function of a policy recursively to be 
\begin{displaymath}
  \begin{split}
    V^{\pi}(s) & = \E_{\pi}\{R_t|s_t=s\}\\
    & = \E_{\pi}\{\sum_{k=0}^{\infty}{\gamma^{k}r_{t+k+1}}|s_t = s\}\\
    & = \E_{\pi}\{r_{t+1}+\gamma \sum_{k=0}^{\infty}{\gamma^{k}r_{t+k+2}}|s_t = s\}\\
  \end{split}
\end{displaymath}
If policy is stochastic, then we have
\begin{displaymath}
  \begin{split}
    V^{\pi}(s) & = \sum_{a}\pi(s,a)\sum_{s'}\Pr(s'|s,a)\big[R(s,a)+\gamma \E_{\pi}\{\sum_{k=0}^{\infty}\gamma^{k}r_{t+k+2}|s_{t+1}=s'\}\big]\\
    & = \sum_{a}\pi(s,a)\sum_{s'}\Pr(s'|s,a)\big[R(s,a)+\gamma V^{\pi}(s')\big]\\
    & = \sum_{a}\pi(s,a)\{R(s,a)+\sum_{s'}\Pr(s'|s,a)\gamma V^{\pi}(s')\}
  \end{split}
\end{displaymath}
If policy is deterministic, then the Value function of policy $\pi$ at state $s$ is
\begin{displaymath}
  V^{\pi}(s) = R(s,\pi(s))+\gamma \sum_{s'}\Pr(s'|s,\pi(s))V^{\pi}(s')
\end{displaymath}
Thus, the Bellman optimality equation can be defined as
\begin{displaymath}
  V^{*}(s) = \max_a \{R(s,a)+\gamma \sum_{s'}\Pr(s'|s,a)V^{*}(s')\}
\end{displaymath}

Detail see \citep{barto1998reinforcement}
\end{topic}

\begin{topic}{bellmanoperator}{Bellman Operator} Consider a \linktopic{MDP}{MDP}, Bellman Operator for policy $\pi$ can be defined as a map from a value function to another value function
\begin{displaymath}
  T^\pi:\mathbb{R}^N \rightarrow \mathbb{R}^N
\end{displaymath}
where $N$ is the cardinality of state space, i.e. $N = |S|$.

\Working
\end{topic}


\begin{topic}{boltzmannselection}{Boltzmann Selection}
\Working
\end{topic}

\begin{topic}{contraction}{Contraction}  The operator $F$ (function $f$) is a $\alpha$-contraction  ($0\le\alpha<1$, called \linktopic{lipcon}{Lipschitz constant} for $F$) with respect to some norm $\|\cdot\|$ if
\begin{displaymath}
  \forall X,\overline{X}: \|FX-F\overline{X}\| \le \alpha \|X-\overline{X}\| \quad \text{or} \quad \|f(X)-f(\overline{X})\| \le \alpha \|X-\overline{X}\|
\end{displaymath}

\begin{itemize}
\item Theorem 1. The sequence $X, FX, F^2X, \ldots$ converges for every $X$.  e.g. $X, f(X), f(f(X)), \ldots$ converges for every $X$\\

Proof:

Useful fact : Cauchy sequences : If for $x_0,x_1,x_2,\ldots,$ we have that
\begin{displaymath}
  \forall \epsilon, \exists K : \|x_M-x_N \| < \epsilon \quad \text{for} \quad M,N > K
\end{displaymath}
then we call $x_0,x_1,x_2,\ldots,$ a Cauchy sequence.

If $x_0,x_1,x_2,\ldots,$ is a Cauchy sequence, and $x_i \in \mathbb{R}^n$, then there exists $x^* \in \mathbb{R}^n$ such that $\lim_{i \to \infty}x_i = x^*$.

Proof:
Assume $N > M$.
\begin{displaymath}
  \begin{alignedat}{2} % use alignedat to align multiple columns
  \| F^MX-F^NX \| & = \| \sum_{i=M}^{N-1}(F^iX-F^{i+1}X)\| & &\\
  & \le \sum_{i=M}^{N-1} \| F^iX-F^{i+1}X \|  & &\quad \text{by triangle inequality}\\
  & \le \sum_{i=M}^{N-1} \alpha ^{i} \| X-FX\|  & &\quad \text{use the condition in Theorem}\\
  & = \| X-FX \| \sum_{i=M}^{N-1}\alpha^i & &\\
  & = \| X-FX \| \frac{\alpha^M}{1-\alpha} & &
  \end{alignedat}
\end{displaymath}

As $\| X-FX \| \frac{\alpha^M}{1-\alpha}$ goes to zero for M going to infinity, we have that for any $\epsilon > 0$ for $\|F^MX-F^NX\| \le \epsilon$ to hold for all $M,N>K$, it suffices to pick $K$ large enough. Hence, $X,FX,\ldots$ is Cauchy sequence and converges.

\item Theorem 2 (Banach fixed-point theorem). $F$ has a unique fixed point $X^*$ which satisfies $FX^* = X^*$ and all
sequences $X$, $FX$, $F^2X$, $\ldots$ converge this unique fixed point $X^âˆ—$.

Proof:

Suppose $F$ has two fixed points. Let's say
\begin{displaymath}
  \begin{split}
  FX_1 = & X_1\\
  FX_2 = & X_2\\
  \end{split}
\end{displaymath}
this implies, $\| FX_1 - FX_2\| = \| X_1-X_2 \|$. At the same time we have from the contractive property of $F$ 
\begin{displaymath}
  \|FX_1-FX_2\| \le \alpha \|X_1-X_2\|
\end{displaymath}
Combining both gives us 
\[
\|X_1-X_2\| \le \alpha\|X_1-X_2\|
\]
Hence, $X_1=X_2$.
\end{itemize}

Detail see \citep{conrad2014contraction}
\end{topic}


\begin{topic}{comfeature}{Compatible Feature} 
If we are using Softmax Policy, which is 
\[
\pi(s,a) = \frac{e^{\phi(s,a)^{\top}\theta}}{\sum_{b}e^{\phi(s,b)^{\top}\theta}}
\]
if we have $p = e^{\phi(s,a)^{\top}\theta}$ and $q = \sum_{b}e^{\phi(s,b)^{\top}\theta}$, then $\pi(s,a) = p/q$
\[
\begin{split}
\nabla_{\theta} \ln \pi(s,a) =\ & \frac{1}{\pi(s,a)} \frac{\partial \pi(s,a)}{\partial \theta}\\
=\ & \frac{q}{p} \frac{\frac{\partial p}{\partial \theta} q - \frac{\partial q}{\partial \theta} p}{q^2}\\
=\ & \frac{\frac{\partial p}{\partial \theta}}{p} - \frac{\frac{\partial q}{\partial \theta}}{q}\\
=\ & \frac{e^{\phi(s,a)^{\top}\theta} \phi(s,a)}{e^{\phi(s,a)^{\top}\theta}} - \frac{\sum_{b} (e^{\phi(s,b)^{\top}\theta} \phi(s,b))}{\sum_{b}e^{\phi(s,b)^{\top}\theta}}\\
=\ & \phi(s,a) - \sum_{b} \frac{e^{\phi(s,b)^{\top}\theta} \phi(s,b)}{\sum_{b}e^{\phi(s,b)^{\top}\theta}}\\
=\ & \phi(s,a) - \sum_{b} \big[\frac{e^{\phi(s,b)^{\top}\theta}}{\sum_{b}e^{\phi(s,b)^{\top}\theta}} \phi(s,b)\big]\\
=\ & \phi(s,a) - \sum_{b} \pi(s,b) \phi(s,b)\\
\end{split}
\]

if we are using $d$-dimensional Gaussian Policy, suppose we fix the covariant matrix to $\mathbf{\Sigma} = c\mathbf{I}$, then our policy is 
\[
\pi(s,a) = \frac{1}{\sqrt{(2 \pi)^{d} |\Sigma|}} \exp\big\{ -\frac{1}{2} (a-\phi(s)^\top\theta)^\top\Sigma^{-1}(a-\phi(s)^\top\theta)\big\}
\]
the score function is 
\[
\begin{split}
\nabla_{\theta} \ln \pi(s,a) =\ & \frac{1}{\pi(s,a)} \frac{\partial \pi(s,a)}{\partial \theta}\\
=\ & \frac{\sqrt{(2 \pi)^d |\Sigma|}}{\exp\big \{ \ldots \big \}}  \frac{\exp\big \{ \ldots \big \}}{\sqrt{(2 \pi)^d |\Sigma|}} (-\frac{1}{2}) \big( \frac{\partial (a-\phi(s)^\top\theta)^\top\Sigma^{-1}(a-\phi(s)^\top\theta)}{\partial \theta}\big)\\
=\ & -\Sigma^{-1} \big[a-\phi(s)^\top \theta \big] \phi(s) 
=
\end{split}
\]
\end{topic}
\begin{topic}{condentropy}{Conditional Entropy}
Suppose we have a joint distribution $p(x, y)$ from which we draw pairs of values of $x$ and $y$. If a value of $x$ is already known, then the additional information needed to specify the corresponding value of $y$ is given by (use the $h(\cdot)$ function in \linktopic{entropy}{entropy})
 \[
 \begin{split}
 h(x,y) - h(x) &= -\ln p(x,y) - (-\ln p(x))\\
&= -\ln p(y|x)p(x) + \ln p(x)\\
&= -\ln p(y|x) -\ln p(x) + \ln p(x)\\
&= -\ln p(y|x) 
 \end{split}
 \]
 Thus the average additional information needed to specify $y$ can be written as
 \[
 H[y|x] = -\iint p(y,x) \ln p(y|x) \D y\D x
 \]
which is called the \textit{\color{red}{conditional entropy}} of $y$ given $x$. It is easily seen, using the product rule, that the conditional entropy satisfies the relation
\[
H[x,y] = H[y|x] + H[x]
\]
where $H[x, y]$ is the \linktopic{dentropy}{differential entropy} of $p(x, y)$ and $H[x]$ is the \linktopic{dentropy}{differential entropy} of the marginal distribution $p(x)$. Thus the information needed to describe $x$ and $y$ is given by the sum of the information needed to describe $x$ alone plus the additional information required to specify $y$ given $x$.

\end{topic}

\begin{topic}{centropy}{Cross Entropy}
	
\end{topic}

\begin{topic}{dentropy}{Differential Entropy}
Differential Entropy generalize the concept of \linktopic{entropy}{entropy} to continuous cases. We first divide $x$ into bins of width $\Delta$. Then, assuming $p(x)$ is continuous, the \textit{mean value theorem} tells us that, for each such bin, there must exist a value $x_i$ such that
\[
\int_{i\Delta}^{(i+1)\Delta} p(x)\D x = p(x_i)\Delta
\]
We can now quantize the continuous variable $x$ by assigning any value $x$ to the value $x_i$ whenever $x$ falls in the $i$th bin. The probability of observing the value $x_i$ is then $p(x_i)\Delta$. This gives a discrete distribution for which the entropy takes the form
\[
\begin{split}
H_{\Delta} & = -\sum_i p(x_i) \Delta \ln(p(x_i)\Delta)\\
& = -\sum_i p(x_i)\Delta \ln p(x_i) - \ln\Delta
\end{split}
\]
If we consider the limit $\Delta \to 0$. The first term will approach the integral of $p(x)\ln p(x)$ in this limit so that 
\[
\lim_{\Delta \to 0}\bigg\{\sum_i p(x_i) \Delta \ln p(x_i) \bigg\} = -\int p(x) \ln p(x) \D x
\]
The term on the very right-hand side is called the \textit{\color{red}{differential entropy}}. We can see that the discrete and continuous forms of the entropy differ by a quantity $\ln \Delta$, which diverges in the limit $\Delta \to 0$. This reflects the fact that to specify a continuous variable very precisely requires a large number of bits.

Let us now consider the maximum entropy configuration for a continuous variable. In order for this maximum to be well defined, it will be necessary to constrain the first and second moments of $p(x)$ as well as preserving the normalization constraint. We therefore maximize the differential entropy with the three constraints
\[
\begin{split}
\int_{-\infty}^{\infty} p(x)\D x & = 1\\
\int_{-\infty}^{\infty} xp(x)\D x & = \mu\\
\int_{-\infty}^{\infty} (x-\mu)^2 p(x)\D x & = \sigma^2\\
\end{split}
\]
The constrained maximization can be performed using Lagrange multipliers so that we maximize the following functional with respect to $p(x)$
\[
-\int_{-\infty}^{\infty} p(x) \ln p(x) \D x + \lambda_1\bigg(\int_{-\infty}^{\infty} p(x)\D x - 1\bigg) + \lambda_2\bigg(\int_{-\infty}^{\infty} xp(x)\D x - \mu\bigg) + \lambda_3\bigg(\int_{-\infty}^{\infty} (x-\mu)^2 p(x)\D x -\sigma^2\bigg)
\]
We set the derivative of this functional to zero giving
\[
p(x) = \exp\{-1+\lambda_1+\lambda_2x+\lambda_3(x-\mu)^2\}
\]

The Lagrange multipliers can be found by back substitution of this result into the three constraint equations, leading finally to the result
\[
p(x) = \frac{1}{(2\pi \sigma^2)^{\frac{1}{2}}}\exp\bigg\{-\frac{(x-\mu)^2}{2\sigma^2}\bigg\}
\]
and so the distribution that maximizes the differential entropy is the Gaussian.
If we evaluate the differential entropy of the Gaussian, we obtain
\[
H[x] = \frac{1}{2}\{1+\ln(2\pi\sigma^2)\}
\]
Thus we see again that the entropy increases as the distribution becomes broader, i.e., as $\sigma^2$ increases. This result also shows that the differential entropy, unlike the discrete entropy, can be negative, because $H(x) < 0$ for $\sigma^2 < \frac{1}{2\pi e}$.
\end{topic}

\begin{topic}{entropy}{Entropy}
Consider a discrete random variable $x$, if we want to use a quantity $h(x)$ to measure how much information is received when we observe a specific value for this variable. The amount of information can be viewed as the ``degree of surprise'' or `` degree of information'' on learning the value of $x$. 

Apparently, if we observed that a highly improbable event has just occurred, we will have received more information than if we observed some very likely event has just occurred, and if we knew that the event was certain to happen we would receive no information. Our measure of information content will therefore depend on the probability distribution $p(x)$, and the quantity $h(x)$ we are looking for is a monotonic function of the probability $p(x)$ that low $p(x)$ indicates high $h(x)$.
 
The form of $h(\cdot)$ can be found by noting that if we have two events $x$ and $y$ that are unrelated, then the information gain from observing both of them should be the sum of the information gained from each of them separately, so that $h(x, y) = h(x) + h(y)$. Two unrelated events will be statistically independent and so $p(x, y) = p(x)p(y)$. Therefore, it is easily shown that $h(x)$ must be given by the logarithm of $p(x)$ and so we have
\[
h(x) = -\log_2 p(x)
\]
The negative sign ensures that information is positive or zero and also the low probability events $x$ correspond to high information content. By adopting the convention in information theory, we use the base of $2$, and in this case, the units of $h(x)$ are bits (`binary digits'). If we use the natural logarithms in defining entropy, the units is `nats' instead of bits, which differ simply by a factor of $\ln 2$.

Now suppose that a sender wishes to transmit the value of a random variable to a receiver. The average amount of information that they transmit in the process is obtained by taking the expectation of $h(x)$ with respect to the distribution p(x) and is given by
\[
H[x] = -\sum_x p(x) \log_2 p(x)
\]
or
\[
H[x] = -\sum_x p(x) \ln p(x)
\]
This important quantity is called the \textit{\color{red}{entropy}} of the random variable $x$. Note that $\lim_{x\to 0}p \ln p = 0$ and so we shall take $p(x) \ln p(x) = 0$ whenever we encounter a value for $x$ such that $p(x) = 0$.  Entropy can be thought as the average amount of information needed to specify the state of a random variable and a sharply peaked distribution $p(x_i)$ will have a relatively low entropy. 


\end{topic}
\begin{topic}{epsilongreedy}{\texorpdfstring{$\epsilon$}--greedy}
% use \texorpdfstring to put math notation in bookmark
$\epsilon$-greedy is a common used exploration strategy for Model-free reinforcement learning. This strategy can be seen as a combination of greedy strategy and random strategy. Every time when agent choose an action to perform, it choose greedily with probability 1-$\epsilon$ (exploitation), and randomly with probability $\epsilon$ (exploration). However, based on the changes of $\epsilon$ value, this method have two versions. First, the $\epsilon$ value can decrease as the learning process goes on, this is the \textit{decay exploration} method. Another one which are more commonly used is that we fix the value of $\epsilon$. 

The difference between those methods is that, the first one can be \hyperlink{glie}{GLIE} if the $\epsilon$-value goes 0 eventually but the second one cannot. Suppose we have a counter of how many times a state have been visited, $n_t(s)$ and a constant $c$. As long as the $\epsilon$ value for the a state $\epsilon_t(s) = \frac{c}{n_t(s)}$ where $0<c<1$, this method can be consider \hyperlink{glie}{GLIE}. However, in practice, we usually use fixed value for $\epsilon$ and after the convergence of the estimation value of action, we switch to greedy selection. 
\end{topic}

\begin{topic}{ergodicmdp}{Ergodic MDP}
An MDP is said to be ergodic if for each policy $\pi$ the Markov chain induced by $\pi$ is ergodic.
We are giving the definition of Ergodicity below. But first, we will give some auxiliary definitions.
Several definition of Markov chain:
\begin{itemize}
\item Reducibility\\
A Markov chain is said to be irreducible if it is possible to get to any state from any state.
\item Aperiodicity\\
A state $i$ has period $k$ if any return to state $i$ must occur in multiples of $k$ time steps. For example, suppose it is possible to return to a state in \{6, 8, 10, 12, $\ldots$\} time steps, then $k$ would be 2, even though 2 does not appear in this list. If $k = 1$, then the state is said to be aperiodic: returns to state $i$ can occur at irregular times.
\item Recurrence\\
A state $i$ is said to be transient if, given that we start in state $i$, there is a non-zero probability that we will never return to $i$. State $i$ is recurrent (or persistent) if it is not transient. Recurrent states are guaranteed to have a finite hitting time.
\end{itemize}
Here we have the definition of Ergodicity.

A state $i$ is said to be ergodic if it is aperiodic and positive recurrent. In other words, a state $i$ is ergodic if it is recurrent, has a period of 1 and it has finite mean recurrence time. If all states in an irreducible Markov chain are ergodic, then the chain is said to be ergodic.

It can be shown that a finite state irreducible Markov chain is ergodic if it has an aperiodic state. {\bf A model has the ergodic property if there's a finite number $N$ such that any state can be reached from any other state in exactly $N$ steps.} For example, we will have $N=1$ if we have a fully connected transition matrix where all transitions have a non-zero probability.
{\bf Additionally, an \linktopic{statdistribution}{stationary distribution} $d^{\pi}$ of states exists and is independent start state $s_0$.}

Detail see \citep{ortner2007linear}.
\end{topic}

\begin{topic}{em}{Expectation Maximization}
The \phrase{expectation maximization} algorithm, or EM algorithm, is a general technique for finding maximum likelihood solutions for probabilistic models having latent variables.
Consider a probabilistic model in which we collectively denote all of the observed variables by $\mat{X}$ and all of the hidden variables by $\mat{Z}$. The joint distribution $p(\mat{X}, \mat{Z}|\theta)$ is governed by a set of parameters denoted $\theta$. Our goal is to maximize the likelihood function that is given by
\[
p(\mat{X}|\theta) = \sum_{\mat{Z}}p(\mat{X},\mat{Z}|\theta)
\]
The discussion is identical if $\mat{Z}$ comprises continuous variables or a combination of discrete and continuous variables, with summation replaced by integration as appropriate.

The EM algorithm can be used in situation where direct optimization of $p(\mat{X}|\theta)$ is difficult, but that optimization of the joint likelihood function $p(\mat{X},\mat{Z}|\theta)$ is significantly easier. Now, we introduce a distribution $q(\mat{Z})$ defined over the latent variables, and we observe that, for any choice of $q(\mat{Z})$, we can have the following derivation
\[
\begin{split}
\ln p(\mat{X},\mat{Z}|\theta) &= \ln p(\mat{Z}|\mat{X},\theta)p(\mat{X},\theta)\\
&= \ln p(\mat{Z}|\mat{X},\theta) + \ln p(\mat{X},\theta)\\
\ln p(\mat{X},\theta) &= \ln p(\mat{X},\mat{Z}|\theta) -  \ln p(\mat{Z}|\mat{X},\theta)\\
\end{split}
\]
Then we times both side $q(\mat{Z})$, and sum over all the $\mat{Z}$.
\[
\sum_{\mat{Z}} q(\mat{Z}) \ln p(\mat{X},\theta) = \sum_{\mat{Z}} q(\mat{Z})\ln p(\mat{X},\mat{Z}|\theta) -  \sum_{\mat{Z}} q(\mat{Z})\ln p(\mat{Z}|\mat{X},\theta)
\]
Since the term $\ln p(\mat{X},\theta)$ does not depend on $\mat{Z}$, we take it out of the summation, and we notice that $\sum_{\mat{Z}}p(\mat{Z}) = 1$. So,
\[
\begin{split}
\ln p(\mat{X},\theta) \sum_{\mat{Z}} q(\mat{Z})  &= \sum_{\mat{Z}} q(\mat{Z})\ln p(\mat{X},\mat{Z}|\theta) -  \sum_{\mat{Z}} q(\mat{Z})\ln p(\mat{Z}|\mat{X},\theta)\\
\ln p(\mat{X},\theta) &= \sum_{\mat{Z}} q(\mat{Z})\ln p(\mat{X},\mat{Z}|\theta) -  \sum_{\mat{Z}} q(\mat{Z})\ln p(\mat{Z}|\mat{X},\theta)\\
\ln p(\mat{X},\theta) &= \sum_{\mat{Z}} q(\mat{Z})\ln p(\mat{X},\mat{Z}|\theta) -  \sum_{\mat{Z}} q(\mat{Z})\ln p(\mat{Z}|\mat{X},\theta) - \sum_{\mat{Z}} q(\mat{Z}) \ln q(\mat{Z}) + \sum_{\mat{Z}} q(\mat{Z}) \ln q(\mat{Z})\\
\ln p(\mat{X},\theta) &= \sum_{\mat{Z}} q(\mat{Z})\ln p(\mat{X},\mat{Z}|\theta) - \sum_{\mat{Z}} q(\mat{Z}) \ln q(\mat{Z}) - \bigg(\sum_{\mat{Z}} q(\mat{Z})\ln p(\mat{Z}|\mat{X},\theta) - \sum_{\mat{Z}} q(\mat{Z}) \ln q(\mat{Z})\bigg)\\
\ln p(\mat{X},\theta) &= \sum_{\mat{Z}} q(\mat{Z})\ln \bigg\{\frac{p(\mat{X},\mat{Z}|\theta)}{q(\mat{Z})}\bigg\}- \bigg(\sum_{\mat{Z}} q(\mat{Z})\ln \bigg\{\frac{p(\mat{Z}|\mat{X},\theta)}{q(\mat{Z})}\bigg\}\bigg)\\
\end{split}
\]
If we define
\[
\begin{split}
\mathcal{L}(q,\theta) &= \sum_{\mat{Z}} q(\mat{Z})\ln \bigg\{\frac{p(\mat{X},\mat{Z}|\theta)}{q(\mat{Z})}\bigg\}\\
\text{KL}(q||p) &= - \sum_{\mat{Z}} q(\mat{Z})\ln \bigg\{\frac{p(\mat{Z}|\mat{X},\theta)}{q(\mat{Z})}\bigg\}
\end{split}
\]
Then, we can have the following equation
\begin{equation}
\label{eq:kldecomposition}
\ln p(\mat{X}|\theta) = \mathcal{L}(q,\theta) + \text{KL}(q||p)
\end{equation}
From~\ref{eq:kldecomposition}, we see that $\text{KL}(q||p)$ is the \linktopic{kl}{Kullback-Leibler divergence} between $q(\mat{Z})$ and the posterior distribution $p(\mat{Z}|\mat{X}, \theta)$. Since Kullback-Leibler divergence satisfies $\text{KL}(q||p)\geq 0$, with equality if, and only if, $q(\mat{Z}) = p(\mat{Z}|\mat{X}, \theta)$. It therefore follows from~\ref{eq:kldecomposition} that $\mathcal{L}(q,\theta) \leq \ln p(\mat{X}|\theta)$, in other words that $\mathcal{L}(q,\theta)$ is a lower bound on $\ln p(\mat{X}|\theta)$.

The EM algorithm is a two-stage iterative optimization technique for finding maximum likelihood solutions. We can use the decomposition~\ref{eq:kldecomposition} to define the EM algorithm and to demonstrate that it does indeed maximize the log likelihood. Suppose that the current value of the parameter vector is $\theta^\text{(old)}$. In the E step, the lower bound $\mathcal{L}(q,\theta^\text{(old)})$ is maximized with respect to $q(\mat{Z})$ while holding $\theta^\text{old}$ fixed. The solution to this maximization problem is easily seen by noting that the value of $\ln p(\mat{X}|\theta^\text{(old)})$ does not depend on $q(\mat{Z})$ and so the largest value of $\mathcal{L}(q,\theta^\text{(old)})$ will occur when the Kullback-Leibler divergence vanishes, in other words when $q(\mat{Z})$ is equal to the posterior distribution $p(\mat{Z}|\mat{X},\theta^\text{(old)})$. In this case, the lower bound will equal the log likelihood.

In the subsequent M step, the distribution $q(\mat{Z})$ is held fixed and the lower bound $\mathcal{L}(q,\theta)$ is maximized with respect to $\theta$ to give some new value $\theta^\text{(new)}$. This will cause the lower bound $\mathcal{L}$ to increase (unless it is already at a maximum), which will necessarily cause the corresponding log likelihood function to increase. Because the distribution $q$ is determined using the old parameter values rather than the new values and is held fixed during the M step, it will not equal the new posterior distribution $p(\mat{Z}|\mat{X},\theta^\text{(new)})$, and hence there will be a nonzero KL divergence. The increase in the log likelihood function is therefore greater than the increase in the lower bound.
\end{topic}



\begin{topic}{replay}{Experience Replay}
\citep{Vanseijen2015Deeper} \citep{Riedmiller2005Neural}
\end{topic}

\begin{topic}{fa}{Function Approximation}
Function approximation is a way of combining supervised learning algorithms with learning procedures in Reinforcement Learning to be able to generalize value function into large scale \linktopic{MDP}{MDP}. There are three different objective function for function approximation. 
\begin{enumerate}
\item Direct method\\
In direct method, we are trying to minimize the square error of our estimation and the ``true'' value function which we temporarily assume given by an oracle.
\[
J(\theta) = \frac{1}{2}(v(s) - V_{\theta}(s))^2
\]
The term$\frac{1}{2}$ is just for neat math like regular supervised learning, and $v(s)$ is the ``true'' value for state $s$ which we want to approximate. To use the gradient descent method, we take the gradient of the $J(\theta)$, which gives us
\[
\begin{split}
\nabla_{\theta} J(\theta) = &\ \frac{1}{2} * 2 * (v(s) - V_{\theta}(s)) \nabla_{\theta} V_{\theta}(s)\\
= &\ (v(s) - V_{\theta}(s)) \nabla_{\theta} V_{\theta}(s)
\end{split}
\]
Note, in the equation above, $v(s)$ is just a constant number. Since we don't really have a oracle in learning, we need to find a way to estimate the ``true'' value function. Various method can be used here. For example, we can use the \linktopic{td}{temporal difference learning} rule, thus the gradient become $(r + \gamma V_{\theta}(s') - V_{\theta}(s)) \nabla_{\theta} V_{\theta}(s)$, or we can use the \linktopic{montecarlo}{monte carlo} estimation, then the gradient become $(G_t - V_{\theta}(s)) \nabla_{\theta} V_{\theta}(s)$ or eligibility trace.

However, this direct method has some issue with convergence. \marginpar{detail here}
\item Fixed point method\\
Another kind of objective function is based on the ``fixed point'' notion in \linktopic{contraction}{contraction}. With the contraction property, we know the value function will converge to a fixed point
\[
	V = TV
\]
where $T$ is the bellman operator. Thus, after convergence, our value function should fulfill the Bellman equation which gives the second objective function
\[
\begin{split}
J(\theta) = &\ \frac{1}{2}(V_{\theta}(s) - TV_{\theta}(s))^2\\
= &\ \frac{1}{2}(V_{\theta}(s)-(r+\gamma V_{\theta}(s')))^2
\end{split}
\]
when we take the gradient of this cost function $J(\theta)$, we have
\[
\begin{split}
\nabla_{\theta} J(\theta) = &\ \frac{1}{2} * 2 * (r+\gamma V_{\theta}(s') - V_{\theta}(s)) \nabla_{\theta} (r+\gamma V_{\theta}(s') - V_{\theta}(s))\\
= &\ (r+\gamma V_{\theta}(s') - V_{\theta}(s)) (\nabla_{\theta} \gamma V_{\theta}(s') - \nabla_{\theta}  V_{\theta}(s))\\
= &\ (r+\gamma V_{\theta}(s') - V_{\theta}(s)) (\gamma \nabla_{\theta} V_{\theta}(s') - \nabla_{\theta}  V_{\theta}(s))
\end{split}
\]

\marginpar{need more stuff here}
\item Projected fixed point method 
\end{enumerate}
\end{topic}


\begin{topic}{gmm}{Gaussian Mixture Model}
By taking linear combinations of more basic distributions such as Gaussians, an probabilistic models known as mixture distributions can be formulated. This linear combination of Gaussians can give rise to very complex densities. By using a sufficient number of Gaussians, and by adjusting their means and covariances as well as the coefficients in the linear combination, almost any continuous density can be approximated to arbitrary accuracy.
We therefore consider a superposition of $K$ Gaussian densities of the form
\begin{equation}
\label{eq:gmm}
p(x) = \sum_{k=1}^{K}\pi_k\normal(x|\,u_k,\Sigma_k)
\end{equation}
which is called a \phrase{Gaussian Mixture Model}. Each Gaussian density $\normal(x|\mu_k,\Sigma_k)$ is called a \phrase{component} of the mixture and has its own mean $\mu_k$ and covariance $\Sigma_k$.

The parameters $\pi_k$ in equation are called \phrase{mixing coefficients}. If we integrate both sides of former equation with respect to $x$, and note that both $p(x)$ and the individual Gaussian components are normalized, we obtain
\[
\begin{split}
	\int p(x) \D x &= \int \sum_{k=1}^{K}\pi_k\normal(x|\mu_k,\Sigma_k) \D x\\
	1 &= \sum_{k=1}^{K}\pi_k \int \normal(x|\mu_k,\Sigma_k) \D x\\
	1 &= \sum_{k=1}^{K}\pi_k
\end{split}
\]
Also, the requirement that $p(x) \geq 0$, together with $\normal(x|\mu_k,\Sigma_k)\geq 0$, implies $\pi_k \geq 0$ for all $k$. Combining this with the condition $\sum_{k=1}^{K}\pi_k = 1$ we obtain 
\[
0 \leq \pi_k \leq 1
\]
We therefore see that the mixing coefficients satisfy the requirements to be probabilities.
From the sum and product rules, the marginal density is given by
\[
p(x) = \sum_{k=1}^{K} p(k) p(x|k)
\]
which is equivalent to the equation~\ref{eq:gmm} in which we can view $\pi_k=p(k)$ as the prior probability of picking the $k^\text{th}$ component, and the density $\normal(x|\mu_k,\Sigma_k) = p(x|k)$ as the probability of $x$ conditioned on $k$. an important role is played by the posterior probabilities $p(k|x)$, which are also known as \phrase{responsibilities}. From Bayes' theorem these are given by
\[
\begin{split}
\gamma_k(x) &= p(k|x)\\
&= \frac{p(k)p(x|k)}{\sum_l p(l)p(x|l)}\\
&= \frac{\pi_k\normal(x|\mu_k,\Sigma_k)}{\sum_l \pi_l \normal(x|\mu_l,\Sigma_l)}
\end{split}
\]
\end{topic}

\begin{topic}{glie}{GLIE}
GLIE stands for ``Greedy in the Limit of Infinite Exploration'' \citep{singh2000convergence}. The learning policies in RL can be divided into two broad categories: a \textit{decay exploration} strategy which become more and more greedy and \textit{persistent exploration} which always maintain a fix exploration rate. The advantage of the first one is that we can eventually converge to the optimal policy. The second one may have the advantage always be adaptive but may not converge to the optimal. (In here, we talk about convergence in the sense that the behavior will become optimal. It is possible that some of the algorithm converge to the correct Q-value but still behave randomly with some probability by using persistent exploration strategy, Q-learning with fix $\epsilon$-greedy for example). We may want to consider this in the context of \linktopic{onoffpolicy}{on-policy\&off-policy}.

If a \textit{decay exploration} strategy has the following two characters:
\begin{enumerate}
\item each action is executed infinitely often in every state that is visited infinitely often, and
\item in the limit, the learning policy is greedy with respect to the Q-value function with probability 1.
\end{enumerate}
Than we can consider this decay exploration strategy GLIE. Some example of GLIE include \linktopic{boltzmannselection}{Boltzmann Selection}, \linktopic{epsilongreedy}{$\epsilon$-greedy}.
\end{topic}

\begin{topic}{kl}{Kullback-Leibler Divergence}
Consider some unknown distribution $p(x)$, and suppose that we have modelled this using an approximating distribution $q(x)$. If we use $q(x)$ to construct a coding scheme for the purpose of transmitting values of $x$ to a receiver (see \linktopic{entropy}{Entropy} for justification of example), then the average additional amount of information (in nats) required to specify the value of $x$ (assuming we choose an efficient coding scheme) as a result of using $q(x)$ instead of the true distribution $p(x)$ is given by

\[
\begin{split}
\text{KL}(p||q) & = - \int p(x) \ln q(x) \D x - \bigg(- \int p(x) \ln p(x) \D x\bigg)\\
& = -\int p(x) \ln \bigg\{\frac{q(x)}{p(x)}\bigg\} \D x\\
& = \int p(x) \ln \bigg\{\frac{p(x)}{q(x)}\bigg\} \D x
\end{split}
\]
This is known as \textit{\color{red}{Kullback-Leibler divergence}} or \textit{\color{red}{relative entropy}} between the distribution $p(x)$ and $q(x)$.

Kullback-Leibler divergence satisfies $\text{KL}(p||q) \ge 0$ with equality if, and only if, $p(x) = q(x)$. Although it is often intuited as a way of measuring the distance between probability distributions, the Kullback-Leibler divergence is not a true metric. It does not obey the triangle inequality, and in general $\text{KL}(P||Q)$ does not equal $\text{KL}(Q||P)$.

KL-divergence can be used to task of density estimation. Suppose that data is being generated from an unknown distribution $p(x)$ that we wish to model. We can try to approximate this distribution using some parametric distribution $q(x|\theta)$, governed by a set of adjustable parameters $\theta$, for example a multivariate Gaussian. One way to determine $\theta$ is to minimize the Kullback-Leibler divergence between $p(x)$ and $q(x|\theta)$ with respect to $\theta$. We cannot do this directly because we don't know $p(x)$. Suppose, however, that we have observed a finite set of training points $x_n$, for $n = 1,\ldots,N$, drawn from $p(x)$. Then the expectation with respect to $p(x)$ can be approximated by a finite sum over these points, so that
\[
\begin{split}
\text{KL}(p||q) &= \int p(x) \ln \bigg\{\frac{p(x)}{q(x|\theta)}\bigg\}\D x\\
&= \int p(x) \{-\ln q(x|\theta) + \ln p(x)\}\D x\\
&= \sum_{n=1}^{N}\{-\ln q(x_n|\theta) + \ln p(x_n)\}\\
\end{split}
\]
The second term on the right-hand side of the equation is independent of $\theta$, and the first term is the negative log likelihood function for $\theta$ under the distribution $q(x|\theta)$ evaluated using the training set. Thus we see that minimizing this Kullback-Leibler divergence is equivalent to maximizing the likelihood function.


\end{topic}

\begin{topic}{importancesampling}{Importance Sampling}
Unlike \linktopic{rejectionsampling}{rejection sampling} or other \linktopic{basicsampling}{sampling} method for standard distribution.
The technique of \phrase{importance sampling} provides a framework for approximating expectations directly but does not itself provide a mechanism for drawing samples from distribution $p(\mat{z})$. Suppose, that it is impractical to sample directly from $p(\mat{z})$ but that we can evaluate $p(\mat{z})$ easily for any given value of $\mat{z}$. Like rejection sampling, we have a proposal distribution $q(\mat{z})$. Given the sample set $\{\mat{z}^{(l)}\}$, then we can use the following derivation
\[
\begin{split}
\E_p[f] &= \int p(\mat{z})f(\mat{z}) \D \mat{z}\\
&= \int p(\mat{z})f(\mat{z}) \frac{q(\mat{z})}{q(\mat{z})}\D \mat{z}\\
&= \int q(\mat{z}) \frac{p(\mat{z})}{q(\mat{z})} f(\mat{z})\D \mat{z}\\
&= \E_q\bigg[\frac{p(\mat{z})}{q(\mat{z})} f\bigg]\\
&= \frac{1}{L} \sum_{l=1}^{L}\frac{p(\mat{z}^{(l)})}{q(\mat{z}^{(l)})}f(\mat{z}^{(l)})
\end{split}
\]
The quantities $r_l = p(\mat{z}^{(l)})/q(\mat{z}^{(l)})$ is the \phrase{importance weights}, and they correct the bias introduced by sampling from the wrong distribution. Unlike rejection sampling, all of the samples generated in importance sampling are retained.

Now consider the case same as in \linktopic{rejectionsampling}{rejection sampling}
\[
p(\mat{z}) = \frac{1}{Z_p}\widetilde{p}(\mat{z})
\]
We make the further assumption that the proposal distribution has the same property
\[
q(\mat{z}) = \frac{1}{Z_q}\widetilde{q}(\mat{z})
\]
We then have
\[
\begin{split}
\E[f] &= \int p(\mat{z})f(\mat{z})\D\mat{z}\\
&= \frac{Z_q}{Z_p}\int f(\mat{z}) \frac{\widetilde{p}(\mat{z})}{\widetilde{q}(\mat{z})} q(\mat{z}) \D\mat{z}\\
&= \frac{Z_q}{Z_p}\frac{1}{L} \sum_{l=1}^{L}\widetilde{r}_l f(\mat{z}^{(l)})
\end{split}
\]
where $\widetilde{r}_l = \widetilde{p}(\mat{z})/\widetilde{q}(\mat{z})$. The ratio of $Z_p/Z_q$ can be evaluate using the same sample set.
\[
\begin{split}
\frac{Z_p}{Z_q} &= \frac{\int \widetilde{p}(\mat{z}) \D \mat{z}}{Z_p} = \int \frac{\widetilde{p}(\mat{z})}{Z_q} \D\mat{z}\\
&= \int \frac{\widetilde{p}(\mat{z})}{\widetilde{q}(\mat{z})/q(\mat{z})} \D\mat{z}\\
&= \int \frac{\widetilde{p}(\mat{z})}{\widetilde{q}(\mat{z})} q(\mat{z})\D\mat{z}\\
&= \frac{1}{L} \sum_{l=1}^{L}\widetilde{r}_l
\end{split}
\]
With this result, we can obtain the value of $\E[f]$.
\[
\begin{split}
\E[f] &= \frac{Z_q}{Z_p}\frac{1}{L} \sum_{l=1}^{L}\widetilde{r}_l f(\mat{z}^{(l)})\\
&= \frac{L}{\sum_{l=1}^{L}\widetilde{r}_l}\frac{1}{L} \sum_{l=1}^{L}\widetilde{r}_l f(\mat{z}^{(l)})\\
&=\sum_{l=1}^{L}\frac{\widetilde{r}_l}{\sum_{l=1}^{L}\widetilde{r}_l} f(\mat{z}^{(l)})\\
&=\sum_{l=1}^{L}w_l f(\mat{z}^{(l)})
\end{split}
\]
where we define
\[
w_l = \frac{\widetilde{r}_l}{\sum_{l=1}^{L}\widetilde{r}_l} = \frac{\widetilde{p}(\mat{z}^{(l)})/\widetilde{q}(\mat{z}^{(l)})}{\sum_{l} \widetilde{p}(\mat{z}^{(l)})/\widetilde{q}(\mat{z}^{(l)})}
\]
As with rejection sampling, the success of the importance sampling approach depends crucially on how well the sampling distribution $q(\mat{z})$ matches the desired distribution $p(\mat{z})$. If, as is often the case, $p(\mat{z})f(\mat{z})$ is strongly varying and has a significant proportion of its mass concentrated over relatively small regions of $\mat{z}$ space, then the set of importance weights $\{r_l\}$ may be dominated by a few weights having large values, with the remaining weights being relatively insignificant. Thus the effective sample size can be much smaller than the apparent sample size $L$. The problem is even more severe if none of the samples falls in the regions where $p(\mat{z})f(\mat{z})$ is large. In that case, the apparent variances of $r_l$ and $r_l f(\mat{z}^{(l)})$ may be small even though the estimate of the expectation may be severely wrong. Hence a major drawback of the importance sampling method is the potential to produce results that are arbitrarily in error and with no diagnostic indication. This also highlights a key requirement for the sampling distribution $q(\mat{z})$, namely that it should not be small or zero in regions where $p(\mat{z})$ may be significant.
\end{topic}

\begin{topic}{IRL}{Inverse Reinforcement Learning} 
Reason for using IRL instead of behavior cloning:
\begin{itemize}
\item Reward functions can generalize the expert's behavior to the state space regions not covered by the demonstration. For example, the transition model can change and thus, the optimal policy change as well. 
\item behavioral cloning cannot be applied whenever the expert demonstrates actions that cannot be executed by the
learner (think at a humanoid robot that learns by observing a human expert)
\end{itemize}
\end{topic}

\begin{topic}{lipcon}{Lipschitz Continuity}
Suppose we are given two \linktopic{metric}{metric space} $(X,\D_X)$ and $(Y,\D_Y)$, where $\D_X$ denotes the metric on the set $X$ and $\D_Y$ is the metric on set $Y$, we have the following definition.
\begin{itemize}
\item A function $f:X\rightarrow Y$ is called \phrase{Lipschitz continuous} at $x\in X$ if there exists a real constant $C\geq 0$ such that, for all $x'$ in $X$,
\[
\D_Y(f(x), f(x')) \le C \D_X(x, x')
\]
\end{itemize}
\citep{Eriksson2013Applied}
\end{topic}

\begin{topic}{lqr}{Linear Quadratic Regulator}
When we consider a special system with linear dynamic and quadratic cost, we have some very nice algorithm so solve this questions.
\[
\begin{split}
	x_{t+1} = &\ A_{t}x_{t} + B_tu_t + w_t    \quad \text{for} \quad t = 0,1,\ldots,N-1\\
	C(x_t,u_t) = &\ x_t^{\top}Q_tx_t + u_t^{\top}R_tu_t
\end{split}
\]
In the equation above, $x_t$ are the state with $n$-dimension, and $u_t$ are the control (action) with $m$-dimension. $A_t$, $B_t$, $Q_t$, $R_t$ are given and have appropriate dimension. $Q_t$ are positive semidefinite symmetric and $R_t$ are positive definite symmetric. $w_t$ is the noise term with given probability distributions that do not depend on $x_t$ ad $u_t$. It also has zero mean and finite variance.

The reason for use this kind of control is that we want to keep the state of the system close to the origin. The quadratic cost function will induce heavy penalty if the system is largely deviate from the it. 
So now we apply Dynamic Programming algorithm to it. First, we have the cost-to-go function of the last state $N$ and other states.
\[
\begin{split}
J_N(x_N) = &\ x_N^{\top}Q_Nx_N\\
J_t(x_t) = &\ \min_{u_k} \E \big\{ x_t^{\top}Q_tx_t + u_t^{\top}R_tu_t + J_{t+1}(A_tx_t + B_tu_t+w_t)\big\}
\end{split}
\]
We can replace the $t$ with $N-1$, then we have 
\[
\begin{split}
J_{N-1}(x_{N-1}) =\ \min_{u_{N-1}} \E \big\{ & x_{N-1}^{\top}Q_{N-1}x_{N-1} + u_{N-1}^{\top}R_{N-1}u_{N-1}\\
 + &(A_{N-1}x_{N-1} + B_{N-1}u_{N-1}+w_{N-1})^{\top}Q_N(A_{N-1}x_{N-1} + B_{N-1}u_{N-1}+w_{N-1})\big\}
\end{split}
\]
Next, we expand the term inside square brackets.
\[
\begin{split}
J_{N-1}(x_{N-1}) = &\  x_{N-1}^{\top}Q_{N-1}x_{N-1} + \min_{u_{N-1}} \big[ u_{N-1}^{\top}R_{N-1}u_{N-1} + u_{N-1}^{\top}B_{N-1}^{\top}Q_NB_{N-1}u_{N-1}\\
 +&\ 2x_{N-1}^{\top}A_{N-1}^{\top}Q_NB_{N-1}u_{N-1}\big] + x_{N-1}^{\top}A_{N-1}^{\top}Q_NA_{N-1}x_{N-1} + \E\big\{ w_{N-1}^{\top}Q_Nw_{N-1}\big\}
\end{split}
\]
Several things to note here. First, some of the terms does not depend on $u_{N-1}$, thus we move them out of the minimization operator. Second, $\E\{w_{N-1}\} = 0$ suggests that $\E\{w_{N-1}^{\top}Q_N(A_{N-1}x_{N-1} + B_{N-1}u_{N-1})\}$ is also zero.
Third, we reverse the position of some terms due to the transpose. If we differentiate the equation with respect to $u_{N-1}$ and set the derivative to zero, we get 
\[
(R_{N-1}+B_{N-1}^{\top}Q_NB_{N-1})u_{N-1} =\ -B_{N-1}^{\top}Q_NA_{N-1}x_{N-1}
\]
Since $R_{N-1}$ is positive definite and $B_{N-1}^{\top}Q_{N-1}B_{N-1}$ is positive semidefinite, matrix multiplying $u_{N-1}$ on the left is positive definite (and hence invertible). Thus, the minimizing control vector is given by
\[
u_{N-1}^{*} =\ -(R_{N-1}+B_{N-1}^{\top}Q_NB_{N-1})^{-1}B_{N-1}^{\top}Q_NA_{N-1}x_{N-1}
\] 
By substitution into the expression for $J_{N-1}$, we have 
\[
J_{N-1}(x_{N-1}) = x_{N-1}^{\top}K_{N-1}x_{N-1} + \E\big\{w_{N-1}^{\top}Q_Nw_{N-1}\big\}
\]
and matrix $K_{N-1}$ should be
\[
K_{N-1} = A_{N-1}^{\top}(Q_N-Q_NB_{N-1}(B_{N-1}^{\top}Q_NB_{N-1}+R_{N-1})^{-1}B_{N-1}^{\top}Q_N)A_{N-1}+Q_{N-1}
\]
\end{topic}







\begin{topic}{lspi}{LSPI}
\Working
\end{topic}

\begin{topic}{lstd}{LSTD}
\Working
Related paper
\end{topic}

\begin{topic}{lstm}{Long-Short Term Memory}
\Working
\end{topic}

\begin{topic}{mcmc}{Markov Chain Monte Carlo}
\Working
\end{topic}

\begin{topic}{MDP}{Markov Decision Process}
blah blah here

Markov Decision Process can be seen as a extension of Markov Chain with additional action set (allowing selection) and reward function (motivation). It can be reduced to Markov chain if we have only one action per state and same reward for all the state.
\end{topic}

\begin{topic}{metric}{Metric Space}
\Working

\textbf{Definition (Metric Space, metric).} A \textit{metric space} is a pair $(X,d)$, where $X$ is a set and $d$ is a metric on $X$ (distance function on $X$), that is, a function defined\footnote[1]{The symbol $\times$ denotes the Cartesian product of sets: $A \times B$ is the set of all ordered pairs $(a,b)$, where $a \in A$, and $b \in B$. Hence, $X \times X$ is the set of all ordered pairs of elements of $X$} on $X \times X$ such that for all $x,y,z \in X$ we have:
\begin{enumerate}
\item $d$ is real-valued, finite and nonnegative.
\item $d(x,y) = 0$ if and only if $x=y$
\item $d(x,y)$ = $d(y,x)$  (Symmetry)
\item $d(x,y) \le d(x,z) + d(z,y)$   (Triangle inequality)
\end{enumerate}

In this definition, $X$ is called the underlying set of $(X,d)$. Its elements are called points. Here are some examples of metric space:
\begin{itemize}
\item Real line $\mathbb{R}$\\
The set of all real numbers and $d(x,y) = |x-y|$
\item Euclidean plane $\mathbb{R}^2$\\
Suppose we have $x = (\epsilon_1,\epsilon_2)$ and $y = (\upsilon_1,\upsilon_2)$. The euclidean metric defined by 
\[
d(x,y) = \sqrt{(\epsilon_1-\upsilon_1)^2+(\epsilon_2-\upsilon_2)^2}
\]
\item Euclidean space $\mathbb{R}^n$\\
If $x = (\epsilon_1,\ldots,\epsilon_n)$ and $y = (\upsilon_1,\ldots,\upsilon_n)$, then euclidean metric defined by 
\[
d(x,y) = \sqrt{(\epsilon_1-\upsilon_1)^2+\ldots+(\epsilon_n-\upsilon_n)^2}
\]
\item Unitary space $\mathbb{C}^n$\\
a n-dimensional unitary space $\mathbb{C}^n$ is the space of all ordered n-tuples of complex numbers with distance function
\[
d(x,y) = \sqrt{|\epsilon_1-\upsilon_1|^2+\ldots+|\epsilon_n-\upsilon_n|^2}
\]
\item Sequence space $l^{\infty}$\\
 This example and the next one give a first impression of how surprisingly the concept of a metric space is. As a set $X$ we take the set of all bounded sequences of complex numbers; that is, every element of $X$ is a complex sequence
 \[
 x = (\epsilon_1,\epsilon_2,\ldots) \quad \text{briefly} \quad  x = (\epsilon_j)
 \]
 such that for all $j=1,2,\ldots$ we have 
\[
|\epsilon_j|\le c_x
\]
where $c_x$ is a real number which may depend on $x$, but does not depend on $j$. We choose the metric defined by
\[
d(x,y) = \sup_{j \in \mathbb{N}}|\epsilon_j-\upsilon_j|
\]
where $y = (\upsilon_j) \in X$ and $\mathbb{N} = {1,2,\ldots}$, and sup denotes the supremum (least upper bound).
 \item Function space $C[a,b]$\\
 As a set $X$ we take the set of all real-valued functions $x,y,\ldots$ which are functions of an independent real variable $t$ and are defined and continuous on a given closed interval $J = [a.b]$. Choosing the metric defined by 
 \[
 d(x,y) = \max_{t\in J}|x(t)-y(t)|
 \]
 where max denotes the maximum, we obtain a metric space which is denoted by $C[a,b]$. (The letter $C$ suggests ``continuous.'') This is a function space because every point of $C[a,b]$ is a function.
\end{itemize}
\citep{Kreyszig1989Introductory}.
\end{topic}

\begin{topic}{norm}{Norm}
a function that assigns  a strictly positive length or size to each vector in a vector space)
\end{topic}

\begin{topic}{montecarlo}{Monte Carlo Method}
Monte Carlo method is a way of making the prediction in model-free environment. The question it wants to solve is that suppose we have a policy $\pi$ known, how good is this policy? In this case, we evaluate the policy by giving the method episodes of experience $\{s_1,a_1,r_2,\ldots,s_T\}$ generated by following policy $\pi$ and wants the value function $V^{\pi}$ as output.

As we know, the value of being in a state $s$ is the expectation of the discounted rewards received afterwards. 
\[
V^{\pi}(s) = \E_{\pi}[r_{t+1} + \gamma r_{t+2} + \ldots + \gamma^{T-1}r_T]
\]

two methods can be used here : first-visit and every-visit method
\end{topic}

\begin{topic}{offpolicygradient}{Off-Policy Policy Gradient Method}
In \linktopic{reinforce}{REINFORCE} method, when we try to evaluate the expected return of a policy $\pi$, we have to run the policy several time to be able to have reasonable estimation of how good the policy is. Let $J(\theta)$ denote the expected return of policy $\pi_{\theta}$, and $\tau$ denote the trajectory (essentially a state-action sequence : $s_0, a_0, s_1, a_1, \ldots, s_T, a_T$) or rollout or history (these terms are interchangeable) of executing the policy $\pi_{\theta}$, then we know
\[
\begin{split}
J(\theta) = & \E_{\pi_{\theta}}\big[\sum_{t=0}^{H}\gamma^tR(s_t,u_t)|\pi_{\theta}\big]\\
= & \sum_{\tau}p(\tau|\theta)R(\tau)
\end{split}
\]
where $P(\tau|\theta)$ is the probability of having a trajectory $\tau$ by following policy $\pi_{\theta}$ and $R(\tau)$ is just the accumulated reward of that trajectory. 
In \linktopic{policy gradient}{policy gradient} method, after we evaluate the policy $\pi_{\theta}$, we may want to improve it and use a new policy. Thus, the sample we collected during the process of following policy $\pi_{\theta}$ are discarded. However, it will preferable if we can reuse the data gathered of following one policy to estimate the value of following another policy. The method ``likelihood ratio'' estimation make this data reuse possible.

In practice, if we want to evaluate $J(\theta)$, we may want to draw the rollout samples from the distribution induced by policy $\pi_{\theta}$. After taking $N$ samples $\{\tau_0,\tau_1,\ldots,\tau_N\}$, we have a unbiased estimator:
\[
	\hat{J(\theta)} = \frac{1}{N}\sum_{i}R(\tau_i)
\]
Imagine, however, instead of $\pi_{\theta}$, we only have $\pi_{\theta'}$ available, we can do some trick like this
\begin{comment}
need to figure why can we ignore the dynamic model
\end{comment}
\[
\begin{split}
J(\theta) = & \sum_{\tau}p(\tau|\theta)R(\tau)\\
= & \sum_{\tau}p(\tau|\theta) \frac{p(\tau|\theta')}{p(\tau|\theta')} R(\tau)\\
= & \sum_{\tau}p(\tau|\theta') \frac{p(\tau|\theta)}{p(\tau|\theta')} R(\tau)\\
= & \E_{\pi_{\theta'}}\bigg[\frac{p(\tau|\theta)}{p(\tau|\theta')} R(\tau)\bigg]
\end{split}
\]
Note that, $p(\tau|\theta)$ and $p(\tau|\theta')$ is the probability of same sample $\tau$ under different distribution induced by $\theta$ and $\theta'$. Now, we can estimate the $J(\theta)$ with respect to the the distribution induced by $\pi_{\theta'}$. This method of estimating one expectation with respect to another distribution is called \linktopic{importancesampling}{importance sampling}, which is widely used for off-policy learning. So, we can rewrite the $\hat{J(\theta)}$ as
\[
\hat{J(\theta)} = \frac{1}{N}\sum_{i}R(\tau_i)\frac{p(\tau|\theta)}{p(\tau|\theta')}
\]
Note, in the equation above, we have a term $\frac{p(\tau|\theta)}{p(\tau|\theta')}$. Since we don't have the model of the world, it's not possible to directly compute $p(\tau|\theta)$ or $p(\tau|\theta')$. But if we expand the fraction term (note that $p(\tau|\theta)$ and $p(\tau|\theta')$ both evaluate on the same trajectory sample $\tau$, therefore the state action sequence is same for both), we can see that 
\[
\begin{split}
\frac{p(\tau|\theta)}{p(\tau|\theta')} = &\ \frac{p(s_0)\prod_{t=0}^T\big[p(s_{t+1}|s_t,a_t)p(a_t|s_t, \theta)\big]}{p(s_0)\prod_{t=0}^T\big[p(s_{t+1}|s_t,a_t)p(a_t|s_t,\theta')\big]}\\
= &\ \frac{p(s_0)\prod_{t=0}^Tp(s_{t+1}|s_t,a_t)\prod_{t=0}^Tp(a_t|s_t, \theta)}{p(s_0)\prod_{t=0}^Tp(s_{t+1}|s_t,a_t)\prod_{t=0}^Tp(a_t|s_t,\theta')}\\
= &\ \frac{\prod_{t=0}^Tp(a_t|s_t,\theta)}{\prod_{t=0}^Tp(a_t|s_t,\theta')}\\
= &\ \frac{\prod_{t=0}^T\pi_{\theta}(a_t|s_t)}{\prod_{t=0}^T\pi_{\theta'}(a_t|s_t)}\\
\end{split}
\]
Thus, we should be able to calculate the likelihood $\frac{p(\tau|\theta)}{p(\tau|\theta')}$ for any two policies $\theta$
 and $\theta'$. Actually, we can have a mixture distributions, where $p(\tau|\theta')$ is replaced by $\frac{1}{N}\sum_jp(\tau_j|\theta_j)$ where $\tau_j$ are trajectory of following $\theta_j$, then we can make use of multiple source of distributions.
\end{topic}

\begin{topic}{onoffpolicy}{On-policy and Off-policy}
An RL algorithm can be essentially divided into two parts, the \textit{learning policy} and \textit{update rule}. The first one is a non-stationary policy that maps experience (state visited, action chosen, reward received) to into a currently choice of action. The second part is how the algorithm uses experience to change its estimation of the optimal value function.
In off-policy algorithm, the \textit{update rules} doesn't have relationship with \textit{learning policy}, that is the \textit{update rules} doesn't care the what action agent take. Q-learning can be consider as the off-policy algorithm.
\begin{displaymath}
  Q_{t+1}(s,a) = (1-\alpha)Q_{t}(s,a)+\alpha(r_t+\gamma \max_{a'}Q(s',a'))
\end{displaymath}

We can see that the Q-value is update based on the $\max_{a'}Q(s',a')$, which doesn't depend on the action the agent was taking.

However, if we take a look of SARSA(0), which is very similar to Q-learning.
\begin{displaymath}
  Q_{t+1}(s,a) = (1-\alpha)Q_{t}(s,a)+\alpha(r_t+\gamma Q(s',a'))
\end{displaymath}

We can see the update is based on the Q-value of the next action of the agent. Thus it is an on-policy algorithm. The convergence condition are heavily depend on the \textit{learning policy}, The Q-value of SARSA(0) can only converge to optimality in the limit only if the learning policy behavior optimal in the limit. The SARSA(0) and Q-learning will be same if we use greedy action selection strategy.

Detail see \citep{singh2000convergence}.
\end{topic}



\begin{topic}{policygradient}{Policy Gradient Reinforcement Learning}
Instead of determining the action based on value function, another method of determine the action to take is the direct policy search method. It is well known that value-function combined with function approximation are unable to converge to any policy even for simple MDPs (in mean-squared-error, residual-gradient, temporal-difference, and dynamic-programming sense) \lackcite 

Let \(\theta\) denote the parameters of the function approximator, neural network for example, and \(J(\theta)\) the performance of the corresponding policy (e.g. the average reward per step). Then in the policy gradient approach, we can update the policy parameters proportional to the gradient:
\[
\Delta\theta = \alpha \frac{\partial J(\theta)}{\partial \theta}
\]
Let's begin with the definition of the two formulations:
\begin{itemize}
\item Average reward formulation\\
\[
J(\theta) = \lim_{n\rightarrow \infty} \frac{1}{n} \E\{r_1 + r_2 + \ldots + r_n | \pi\} = \sum_s d^{\pi}(s) \sum_a \pi(s,a) R(s,a)
\]
where \(d^{\pi}(s)\) is the stationary distribution of the states induced by \(\pi\)
\end{itemize}
where \(\alpha\) is a positive-definite step size.
\end{topic}

\begin{topic}{pgt}{Policy Gradient Theorem} 
Using the same notation in \linktopic{policygradient}{Policy Gradient}, 
\end{topic}

\begin{topic}{pi}{Policy Iteration}
\Working
\end{topic}

\begin{topic}{reinforce}{REINFORCE}
REINFORCE algorithm also finds an unbiased estimate of
the gradient, but without the assistance of a learned value function. REINFORCE
learns much more slowly than RL methods using value functions and has received
relatively little attention. Learning a value function and using it to reduce the variance
of the gradient estimate appears to be essential for rapid learning.

Likelihood Ratio Methods
\[
\begin{split}
	J(\theta) = & \E \big[r(\tau)\big] \\
	= &\ \int_{\tau} p_{\theta}(\tau)\ r(\tau)\\
	\nabla_{\theta} J(\theta) = & \nabla_{\theta} \int_{\tau} p_{\theta}(\tau)\ r(\tau)\\
	= &\ \int_{\tau} \nabla_{\theta} p_{\theta}(\tau)\ r(\tau)\\
	= &\ \int_{\tau} \nabla_{\theta} p_{\theta}(\tau)\ \frac{p_{\theta}(\tau)}{p_{\theta}(\tau)}\ r(\tau)\\
	= &\ \int_{\tau} p_{\theta}(\tau)\ \frac{\nabla_{\theta} p_{\theta}(\tau)}{p_{\theta}(\tau)}\ r(\tau)\\
	= &\ \int_{\tau} p_{\theta}(\tau)\ \nabla_{\theta} \ln p_{\theta}(\tau)\ r(\tau)\\
	= &\ \E \big[ \nabla_{\theta} \ln p_{\theta}(\tau)\ r(\tau) \big]
\end{split}
\]
However, we know that
\[
p_{\theta}(\tau) = p(x_0) \prod_{k=0}^{H}p(x_{k+1}|x_k,u_k)\pi_{\theta}(u_k|x_k)
\]
Thus
\[
\begin{split}
\nabla_{\theta} \ln p_{\theta}(\tau) = &\ \nabla_{\theta} \big[ \ln p(x_0) + \sum_{k=0}^{H} (\ln p(x_{k+1}|x_k,u_k) + \ln \pi_{\theta}(u_k|x_k))\big]\\
= &\ \nabla_{\theta} \big[ \ln p(x_0) + \sum_{k=0}^{H} \ln p(x_{k+1}|x_k,u_k) + \sum_{k=0}^{H} \ln \pi_{\theta}(u_k|x_k)\big]\\
= &\ \nabla_{\theta} \ln p(x_0) + \sum_{k=0}^{H} \nabla_{\theta} \ln p(x_{k+1}|x_k,u_k) + \sum_{k=0}^{H} \nabla_{\theta} \ln \pi_{\theta}(u_k|x_k)\\
= &\ 0 + 0 + \sum_{k=0}^{H} \nabla_{\theta} \ln \pi_{\theta}(u_k|x_k)\\
= &\ \sum_{k=0}^{H} \nabla_{\theta} \ln \pi_{\theta}(u_k|x_k)\\
\end{split}
\]
Note, if instead of stochastic policy, we are using a deterministic policy, then we have 
\[
p_{\theta}(\tau) = p(x_0) \prod_{k=0}^{H}p(x_{k+1}|x_k,\pi_{\theta}(x_k))
\]
In this case, 
\[
\begin{split}
\nabla_{\theta} \ln p_{\theta}(\tau) = &\ \nabla_{\theta} \big[ \ln p(x_0) + \sum_{k=0}^{H} \ln p(x_{k+1}|x_k,\pi_{\theta}(x_k))\big]\\
= &\ \nabla_{\theta} \ln p(x_0) + \sum_{k=0}^{H} \nabla_{\theta} \ln p(x_{k+1}|x_k,\pi_{\theta}(x_k))\\
= &\ \nabla_{\theta} \ln p(x_0) + \sum_{k=0}^{H} \nabla_{u_k} \ln p(x_{k+1}|x_k,u_k) \nabla_{\theta} \pi_{\theta}(x_k)\\
= &\ 0 +  \sum_{k=0}^{H} \nabla_{u_k} \ln p(x_{k+1}|x_k,u_k) \nabla_{\theta} \pi(x_k)\\
= &\ \sum_{k=0}^{H} \nabla_{u_k} \ln p(x_{k+1}|x_k,u_k) \nabla_{\theta} \pi(x_k)\\
\end{split}
\]
Since we need to compute $\nabla_{u_k} \ln p(x_{k+1}|x_k,u_k)$, thus we need to know the model of the system
\end{topic}

\begin{topic}{rejectionsampling}{Rejection Sampling}
Due some constraints, the inverse cumulative distribution function (CDF) is intractable (no analytic form is available through the indefinite integral) for some distributions. Therefore, direct sampling from the this kind of posterior distributions in probabilistic model cannot use the technique described in \linktopic{basicsampling}{basic sampling}. Furthermore, it is often the case that for a given distribution 
\[
p(\mat{z}) = \frac{1}{Z_p}\widetilde{p}(\mat{z})
\]
evaluating $\widetilde{p}(\mat{z})$ for any given value of $\mat{z}$ is easy, but to compute the normalizing constant $Z_p$ is hard. In practice, we may only know about the part that is directly associate with $z$, and we treat the rest part as $Z_p$. For example, consider a gamma distribution
\[
\text{Gam}(z|a,b) = \frac{b^a z^{a-1} \exp(-bz)}{\Gamma(a)}
\]
The expression $\frac{b^a}{\Gamma(a)}$ is not relevant to $z$, so we can treat it as $Z_p$. For the cases where the vale of $Z_p$
 is hard to obtain, we want to use the sampling methods only require the knowledge of $\widetilde{p}(z)$, thus, avoid the computation of $Z_p$.
 
In order to apply rejection sampling, we need some simpler distribution q(z), sometimes called a \phrase{proposal distribution}, from which we can readily draw samples. We next introduce a constant $k$ whose value is chosen such that $kq(z)\geq \widetilde{p}(z)$ for all values of $z$. The function $kq(z)$ is called the \phrase{comparison function}. Each step of the rejection sampler involves generating two random numbers. First, we generate a number $z_0$ from the distribution $q(z)$. Next, we generate a number $u_0$ from the uniform distribution over $[0,kq(z_0)]$. This pair of random numbers has uniform distribution under the curve of the function $kq(z)$. Finally, if $u_0 > \widetilde{p}(z_0)$ then the sample is rejected, otherwise $u_0$ is retained. Thus the pair is rejected if it doesn't lie in the region in $\widetilde{p}(z)$. The remaining pairs then have uniform distribution under the curve of $\widetilde{p}(z)$, and hence the correspond $z$ values are distributed according to $p(z)$, as desired. 

To see why the obtained sample obeying $p(z)$, we first notice that the probability of accept the sample is given by
\begin{equation}
\label{eq:rejectionaccept}
p(\text{accept}|\mat{z}) = \frac{\widetilde{p}(\mat{z})}{kq(\mat{z})}
\end{equation}
Thus, the probability of drawing a sample $\mat{z}$ following the proposed method is
\[
q(\mat{z}) p(\text{accept}|\mat{z}) = q(\mat{z}) \frac{\widetilde{p}(\mat{z})}{kq(\mat{z})} = \frac{\widetilde{p}(\mat{z})}{k}
\]
which is a probability density that integrates to 1
\[
\int \frac{\widetilde{p}(\mat{z})}{k} \D \mat{z} = 1
\]
Since $p(\mat{z})$ is $\widetilde{p}(\mat{z})$ normalized with $Z_p$,
\[
p(\mat{z}) = \frac{\widetilde{p}(\mat{z})}{Z_p}
\]
where
\[
\int \frac{\widetilde{p}(\mat{z})}{Z_p} \D \mat{z} = 1
\]
Therefore, $k = Z_p$, and $p(\mat{z})$ is $\frac{\widetilde{p}(\mat{z})}{k}$ which is normalized.

From equation~(\ref{eq:rejectionaccept}), we see that the fraction of points that are rejected by this method depends on the ratio of the area under the unnormalized distribution, $p(z)$ to the area under the curve $kq(z)$. We therefore see that the constant $k$ should be as small as possible subject to the limitation that $kq(z)$ must be nowhere less than $\widetilde{p}(z)$.

In many instances where we might wish to apply rejection sampling, it proves difficult to determine a suitable analytic form for the envelope distribution $q(z)$. An alternative approach is to construct the envelope function on the fly based on measured values of the distribution $p(z)$. Construction of an envelope function is particularly straightforward for cases in which $p(z)$ is log concave, in other words when $-\ln p(z)$ is a convex function.

The function $\ln p(z)$ and its gradient are evaluated at some initial set of grid points, and the intersections of the resulting tangent lines are used to construct the envelope function. Next a sample value is drawn from the envelope distribution. This is straightforward because the log of the envelope distribution is a succession of linear functions, and hence the envelope distribution itself comprises a piecewise exponential distribution of the form
\[
q(z) = k_i \lambda_i \exp\{-\lambda_i (z - z_{i-1})\}
\]
Once a sample has been drawn, the usual rejection criterion can be applied. If the sample is accepted, then it will be a draw from the desired distribution. If, however, the sample is rejected, then it is incorporated into the set of grid points, a new tangent line is computed, and the envelope function is thereby refined. As the number of grid points increases, so the envelope function becomes a better approximation of the desired distribution $p(z)$ and the probability of rejection decreases. This is called \phrase{adaptive rejection sampling}.

Clearly for rejection sampling to be of practical value, we require that the comparison function be close to the required distribution so that the rate of rejection is kept to a minimum. However, even with a great comparison function, the acceptance rate can still diminishe exponentially with the increasing of dimensionality. Thus, this method is more suitable for problems with one or two dimensions.
\end{topic}

\begin{topic}{sir}{Sampling-Importance-Resampling}
The \linktopic{rejectionsampling}{rejection sampling} method depends in part for its success on the determination of a suitable value for the constant $k$. For many pairs of distributions $p(\mat{z})$ and $q(\mat{z})$, it will be impractical to determine a suitable value for k in that any value that is sufficiently large to guarantee a bound on the desired distribution will lead to impractically small acceptance rates.

As in the case of rejection sampling, the \phrase{sampling-importance-resampling} (SIR) approach also makes use of a sampling distribution $q(\mat{z})$ but avoids having to determine the constant $k$. There are two stages to the scheme. In the first stage, $L$ samples $\mat{z}^{(1)}, \ldots , \mat{z}^{(L)}$ are drawn from $q(\mat{z})$. Then in the second stage, weights $w_1, \ldots, w_L$ are constructed using the method from \linktopic{importancesampling}{importance sampling}. Finally, a second set of $L$ samples is drawn from previous sample set $(\mat{z}^{(1)} , \ldots , \mat{z}^{(L)} )$ with probabilities given by the weights $(w_1, \ldots , w_L)$.

The resulting $L$ samples are only approximately distributed according to $p(\mat{z})$, but the distribution becomes correct in the limit $L \to \infty$. To see this, consider the univariate case, and note that the cumulative distribution of the resampled values is given by

\Working
\end{topic}

\begin{topic}{statdistribution}{Stationary Distribution}
\Working
\end{topic}

\begin{topic}{stogame}{Stochastic Game}
Stochastic game can been seen as an extension of \linktopic{MDP}{MDP}.
\end{topic}

\begin{topic}{td}{Temporal Difference Method} 
\end{topic}

\begin{topic}{universal}{Universal Approximator} 
Accroding to \citep{hornik1989multilayer}, Multilayer feedforward networks are universal approximators. Single hidden layer  feedforward networks can approximate any measurable function arbitrarily well regardless of the activation function $\Psi$, the dimension of the input space $r$, and the input space environment $\mu$
\begin{enumerate}
\item universal approximators: standard multilayer feedforward networks are capable of approximating
any measurable function to any desired degree of accuracy
\item there are no theoretical constraints for the success of
feedforward networks
\item lack of success is due to inadequate learning, insufficient number of hidden units or the lack of a deterministic relationship between input and target
\item rate of convergence as the number of hidden units grows
\item rate of increase of the number of hidden units as the input dimension increases for a fixed accuracy
\end{enumerate}
\end{topic}

\begin{topic}{vi}{Value Iteration} 
\Working
\end{topic}

\begin{topic}{variational}{Variational Inference}
Instead of using \linktopic{basicsampling}{sampling method} to approximate the posterior distribution of $p(\mat{Z}|\mat{X})$, we could use the distribution we come up with to approximate the real posterior. This method of using one distribution to approximate another distribution is \phrase{variational inference}, which is the deterministic way of doing \linktopic{approxinfer}{approximate inference} method.

Suppose we have a fully Bayesian model in which all parameters are given prior distributions. The model may also have latent variables as well as parameters, and we shall denote the set of all latent variables and parameters by $\mat{Z}$. Similarly, we denote the set of all observed variables by $\mat{X}$. For example, we might have a set of $N$ independent, identically distributed data, for which $X = \{\mat{x}_1,\ldots,\mat{x}_N\}$ and $Z = \{\mat{z}_1,\ldots,\mat{z}_N\}$. Our probabilistic model specifies the joint distribution $p(\mat{X}, \mat{Z})$, and our goal is to find an approximation for the posterior distribution $p(\mat{Z}|\mat{X})$ as well as for the model evidence $p(\mat{X})$. Similar to \linktopic{em}{EM}, we can decompose the log marginal probability using
\[
\begin{split}
\mathcal{L}(q) &= \int_{\mat{Z}} q(\mat{Z})\ln \bigg\{\frac{p(\mat{X},\mat{Z})}{q(\mat{Z})}\bigg\} \D \mat{Z}\\
\text{KL}(q||p) &= - \int_{\mat{Z}} q(\mat{Z})\ln \bigg\{\frac{p(\mat{Z}|\mat{X})}{q(\mat{Z})}\bigg\}\D\mat{Z}
\end{split}
\]
Note the difference here is that the parameter vector $\theta$ no longer appears, because now the parameters are now stochastic variables which is part of $\mat{Z}$. As before, we can maximize the lower bound L(q) by optimization with respect to the distribution q(Z), which is equivalent to minimizing the KL divergence. If we allow any possible choice for q(Z), then the maximum of the lower bound occurs when the KL diver- gence vanishes, which occurs when q(Z) equals the posterior distribution p(Z|X). However, we shall suppose the model is such that working with the true posterior distribution is intractable.

We therefore consider instead a restricted family of distributions q(Z) and then seek the member of this family for which the KL divergence is minimized. Our goal is to restrict the family sufficiently that they comprise only tractable distributions, while at the same time allowing the family to be sufficiently rich and flexible that it can provide a good approximation to the true posterior distribution. It is important to emphasize that the restriction is imposed purely to achieve tractability, and that sub- ject to this requirement we should use as rich a family of approximating distributions as possible. In particular, there is no ?over-fitting? associated with highly flexible dis- tributions. Using more flexible approximations simply allows us to approach the true posterior distribution more closely.
\end{topic}

\begin{topic}{vs}{Vector Space}
\Working
\end{topic}

\bibliography{RL}
\end{document}




